{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to SparkR Exercise Notebook\n",
    "\n",
    "Hello and welcome to your Hands-on Lab notebook. In this notebook, you will find code examples and exercises for you to practice SparkR and understand its functionalities.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">If you did not complete the Jupyter Notebooks tutorials, it is highly recommendable you do so before utilizing this notebook. You can find them at your **welcome page**.</div>\n",
    "\n",
    "## Table of Contents\n",
    "\n",
    "[Hands-on Lab 1: Getting Started with SparkR](#hl1)\n",
    "  - [Initializing SQL Context](#sql)\n",
    "  - [Importing Data to SparkR](#import)\n",
    "  - [Regarding data frames](#tables)\n",
    "  - [Registering data frames as tempTables](#temp)\n",
    "  - [Useful functions: head and printSchema](#use1)\n",
    "\n",
    "[Hands-on Lab 2: Data Manipulation in SparkR](#hl2)\n",
    "  - [Selecting Columns](#selcol)\n",
    "  - [Filtering by Condition](#conditions)\n",
    "  - [Grouping by Average, Sum and Count](#groupby)\n",
    "  - [Operating on Columns](#oper)\n",
    "  - [Utilizing SQL Queries in SparkR](#queries)\n",
    "  \n",
    "[Hands-on Lab 3: Linear Models in SparkR](#hl3)\n",
    "  - [Creating a Gaussian Regression Model](#gauss)\n",
    "  - [Creating a Binomial Regression Model](#binomial)\n",
    "  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lab 1: Getting Started with SparkR <a id=\"hl1\"></a>\n",
    "\n",
    "In this Hands-on Lab, we will be going over the basic syntax and functionalities of SparkR. To begin using SparkR, you need first to properly install and load the libraries. However (thankfully), Jupyter Notebooks has SparkR installed and configured already, since it uses the IRKernel. The only thing we need to do beforehand is initializing the **SQL Context for SparkR**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">**NOTE:** If you are not running code through Data Scientist Workbench, such as your own instance of Jupyter Notebooks, you will probably need to initialize the **R environment**. To do so, execute the following line of code. Note that if you are running from the SparkR shell, you do not need to execute this.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-using existing Spark Context. Please stop SparkR with sparkR.stop() or restart R to create a new Spark Context\n"
     ]
    }
   ],
   "source": [
    "#Initialize the R enRvironment\n",
    "#Only needed if you run this outside Data Scientist Workbench or a SparkR shell\n",
    "sc <- sparkR.init()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initializing the SQL Context \n",
    "sqlContext enables SparkR to read and manipulate structured data. As such, it is very important to start it up whenever you are utilizing SparkR. You can do this by execute the code in the cell below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Executing this creates a SQL context using SparkR context\n",
    "#Make the name of the variable something you can remember, as you'll need the SQL context for most functions\n",
    "sqlContext <- sparkRSQL.init(sc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Importing Data to SparkR\n",
    "Now that we have initialized the SQL Context, we need data to load up into SparkR. For the purposes of this notebook, examples will be provided utilizing the `mtcars` dataset provided by R and the exercises will utilize `iris`, another local R dataset.\n",
    "\n",
    "If you want to use one of your datasets, feel free to drag it to Jupyter Notebooks' data tab. You can get a link to it by selecting it in the Recent Data tab and then clicking `Insert Path`. Use this path in whatever function is adequate for the format of your file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Regarding Data Frames\n",
    "Dataframes are SparkR's prime data structure. Data Frames are used for storing, manipulating, and organizing data. There are a few ways to create a Data Frame in SparkR. You can utilize the `createDataFrame` function, if there's already a local **R** data frame in place, you can use `read.df` if your file is of a format natively readable by SparkR (such as a correctly structured JSON file or Parquet), or you can take a look in the <a href=\"http://spark-packages.org/\">Spark Packages</a> and see if there is any packages made for reading your file.\n",
    "\n",
    "To read and create data from our `mtcars` dataset, we use the `createDataFrame` function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a data frame called \"cars\" using R's native dataset \"mtcars\"\n",
    "cars <- createDataFrame(sqlContext, mtcars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, you do the same for the `iris` dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In FUN(X[[i]], ...): Use Sepal_Length instead of Sepal.Length  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Sepal_Width instead of Sepal.Width  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Petal_Length instead of Petal.Length  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Petal_Width instead of Petal.Width  as column name"
     ]
    }
   ],
   "source": [
    "#Create a data frame called \"flowers\" using R's native dataset \"iris\"\n",
    "#type your code here\n",
    "#Create a data frame called \"cars\" using R's native dataset \"mtcars\"\n",
    "flowers <- createDataFrame(sqlContext, iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">You might receive some warning messages regarding the `iris` dataset. This is due to the column names not complying to the naming guidelines - for the purposes of this notebook, you can ignore them.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Registering Data Frames as tempTables\n",
    "One of SparkR's more unique features is the capability to perform SQL queries on Data Frames. To do so, you generate a temporary SQL table (the so called `tempTable`) in Spark. We will go over performing SQL queries in the next Lab.\n",
    "\n",
    "For now, to register a temporary SQL table, we use the following function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a temporary SQL table called \"cars\" using our SparkR data frame \"cars\"\n",
    "registerTempTable(cars,\"cars\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the same for the `flowers` data frame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a temporary SQL table called \"flowers\" using our SparkR data frame \"flowers\"\n",
    "#type your code here\n",
    "\n",
    "registerTempTable(flowers,\"flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Useful functions: head and printSchema\n",
    "Now that you have your structured data ready for SparkR, you can take a look over your data with some handy functions. The datasets you use might be very large, and as such, printing the entire data frame might be a little too messy. In this case, you can use the `head` function to take a look at only the first six rows, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n",
       "1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n",
       "2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n",
       "3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n",
       "4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n",
       "5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n",
       "6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the first six rows of our \"cars\" SparkR data frame\n",
    "#You need the SparkR:: prefix due to R already having a head function\n",
    "SparkR::head(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can also take a look at the typing scheme of your data frame using the `printSchema` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- mpg: double (nullable = true)\n",
      " |-- cyl: double (nullable = true)\n",
      " |-- disp: double (nullable = true)\n",
      " |-- hp: double (nullable = true)\n",
      " |-- drat: double (nullable = true)\n",
      " |-- wt: double (nullable = true)\n",
      " |-- qsec: double (nullable = true)\n",
      " |-- vs: double (nullable = true)\n",
      " |-- am: double (nullable = true)\n",
      " |-- gear: double (nullable = true)\n",
      " |-- carb: double (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "#Look at the schema for our SparkR data frame \"cars\"\n",
    "printSchema(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try doing the same for your `flowers` data frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n",
       "1          5.1         3.5          1.4         0.2  setosa\n",
       "2          4.9         3.0          1.4         0.2  setosa\n",
       "3          4.7         3.2          1.3         0.2  setosa\n",
       "4          4.6         3.1          1.5         0.2  setosa\n",
       "5          5.0         3.6          1.4         0.2  setosa\n",
       "6          5.4         3.9          1.7         0.4  setosa"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Look at the first six rows of our \"flowers\" SparkR data frame\n",
    "#type your code here\n",
    "SparkR::head(flowers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- Sepal_Length: double (nullable = true)\n",
      " |-- Sepal_Width: double (nullable = true)\n",
      " |-- Petal_Length: double (nullable = true)\n",
      " |-- Petal_Width: double (nullable = true)\n",
      " |-- Species: string (nullable = true)\n"
     ]
    }
   ],
   "source": [
    "#Look at the schema for our SparkR data frame \"flowers\"\n",
    "#type your code here\n",
    "printSchema(flowers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">That wraps up our Hands-on Lab 1. Remember that if you want any more information on SparkR, you can visit their documentation page, at https://spark.apache.org/docs/latest/sparkr.html</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Hands-on Lab 2: Data Manipulation in SparkR <a id=\"hl2\"></a>\n",
    "In this Hands-on Lab, we will go over methods to **select, filter, and manipulate Data Frames**. The manipulation of Data Frames is the cornerstone of SparkR, and as such, it's a good thing to practice whenever you can.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">This Hands-on Lab assumes you have already created the SQL Context and loaded the `mtcars` and `iris` data frames from R. If for some reason you have not done so yet, execute the code block below.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In FUN(X[[i]], ...): Use Sepal_Length instead of Sepal.Length  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Sepal_Width instead of Sepal.Width  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Petal_Length instead of Petal.Length  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Petal_Width instead of Petal.Width  as column name"
     ]
    }
   ],
   "source": [
    "#Initiate our SQL Context\n",
    "sqlContext <- sparkRSQL.init(sc)\n",
    "#Create a Data Frame called \"cars\" from the mtcars dataset\n",
    "cars <- createDataFrame(sqlContext,mtcars)\n",
    "#Create a Data Frame called \"flowers\" from the iris dataset\n",
    "flowers <- createDataFrame(sqlContext,iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting Columns\n",
    "All of our data frames are separated in **rows and columns**, much like a data table. Most of the time, we would want to retrieve values from a specific column. To do this, we use the `select` function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   mpg\n",
       "1 21.0\n",
       "2 21.0\n",
       "3 22.8\n",
       "4 21.4\n",
       "5 18.7\n",
       "6 18.1"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select from the \"cars\" data frame the \"mpg\" column\n",
    "#select(cars,cars$mpg)\n",
    "#Select the first six rows of the \"mpg\" column from the \"cars\" data frame\n",
    "#Remember that you have to add the SparkR:: prefix to head since R already has an incompatible head function\n",
    "SparkR::head(select(cars,cars$mpg))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, do the same for the `flowers` data frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Petal_Length\n",
       "1          1.4\n",
       "2          1.4\n",
       "3          1.3\n",
       "4          1.5\n",
       "5          1.4\n",
       "6          1.7"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the first six rows of the \"Petal_Length\" column from the \"flowers\" data frame\n",
    "#type your code here\n",
    "SparkR::head(select(flowers,flowers$Petal_Length))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering by Conditions\n",
    "You can also **filter rows by imposing conditions** upon given columns. This something critical to know how to do, for you may want to subset your data frame given certain condition. For this, you use the `filter` function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Select the first six rows of \"cars\" that have a value under 20 in the \"mpg\" column\n",
    "#We have to use the SparkR:: prefix since R already has a conflicting filter function\n",
    "SparkR::head(SparkR::filter(cars, cars$mpg < 20))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try doing the same for the `flowers` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  Sepal_Length Sepal_Width Petal_Length Petal_Width Species\n",
       "1          4.7         3.2          1.3         0.2  setosa\n",
       "2          4.3         3.0          1.1         0.1  setosa\n",
       "3          5.8         4.0          1.2         0.2  setosa\n",
       "4          5.4         3.9          1.3         0.4  setosa\n",
       "5          4.6         3.6          1.0         0.2  setosa\n",
       "6          5.0         3.2          1.2         0.2  setosa"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the first six rows of \"flowers\" that have a value under 1.4 in the \"Petal_Length\" column\n",
    "#type your code here\n",
    "SparkR::head(SparkR::filter(flowers, flowers$Petal_Length < 1.4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Grouping by Average, Sum and Count\n",
    "Another useful function is **grouping your data frame rows by their average, sum, and count**. This enables you to create a histogram or generate other relevant information with great ease. This is done with the `summarize` and `groupby` functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   mpg count\n",
       "1 21.0     2\n",
       "2 33.9     1\n",
       "3 19.2     2\n",
       "4 32.4     1\n",
       "5 15.0     1\n",
       "6 21.4     2"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   mpg  sum\n",
       "1 21.0 42.0\n",
       "2 33.9 33.9\n",
       "3 19.2 38.4\n",
       "4 32.4 32.4\n",
       "5 15.0 15.0\n",
       "6 21.4 42.8"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   mpg average\n",
       "1 21.0   110.0\n",
       "2 33.9    65.0\n",
       "3 19.2   149.0\n",
       "4 32.4    66.0\n",
       "5 15.0   335.0\n",
       "6 21.4   109.5"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the first six elements of the grouping of \"cars\" by its \"mpg\" column, plus the count of the occurrances of that given\n",
    "#\"mpg\" value in the dataset\n",
    "SparkR::head(summarize(groupBy(cars, cars$mpg), count = n(cars$mpg)))\n",
    "#Select the first six elements of the grouping of \"cars\" by its \"mpg\" column, plus the sum of all occurrances of that given\n",
    "#\"mpg\" value in the dataset\n",
    "SparkR::head(summarize(groupBy(cars, cars$mpg), sum = sum(cars$mpg)))\n",
    "#Select the first six elements of the grouping of \"cars\" by its \"mpg\" column, plus the average of all \"hp\" column values\n",
    "#in rows which have that given \"mpg\" value\n",
    "SparkR::head(summarize(groupBy(cars, cars$mpg), average = avg(cars$hp)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you can also sort the data using the `arrange` function, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   mpg average\n",
       "1 15.0     335\n",
       "2 15.8     264\n",
       "3 13.3     245\n",
       "4 14.3     245\n",
       "5 14.7     230\n",
       "6 10.4     210"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Make a variable called \"group\" which is the grouping of \"cars\" by its \"mpg\" column, plus the average of all \"hp\" column values \n",
    "#in rows which have that given \"mpg\" value\n",
    "group <- summarize(groupBy(cars, cars$mpg), average = avg(cars$hp))\n",
    "#Take the first six elements of \"group\" which are ordered in decreasing \"average\" column value order\n",
    "SparkR::head(arrange(group, desc(group$average)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try it on the `flowers` data frame!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Make a variable called \"petals\" which is the grouping of \"flowers\" by its \"Petal_Length\" column, plus the count of its occurrances\n",
    "#type your code here\n",
    "#Take first six elements of \"petals\" which are ordered in decreasing \"count\" column order\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Operating on Columns\n",
    "Now that you know how to select columns, you can now **perform operations on them**. Virtually any function in SparkR can be applied to a column. To do so, you use the `$` operator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "       mpg cyl disp  hp drat    wt  qsec vs am gear carb\n",
       "1 5.547613   6  160 110 3.90 2.620 16.46  0  1    4    4\n",
       "2 5.547613   6  160 110 3.90 2.875 17.02  0  1    4    4\n",
       "3 6.023123   4  108  93 3.85 2.320 18.61  1  1    4    1\n",
       "4 5.653282   6  258 110 3.08 3.215 19.44  1  0    3    1\n",
       "5 4.940017   8  360 175 3.15 3.440 17.02  0  0    3    2\n",
       "6 4.781514   6  225 105 2.76 3.460 20.22  1  0    3    1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "   mpg cyl disp  hp drat    wt  qsec vs am gear carb\n",
       "1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\n",
       "2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\n",
       "3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\n",
       "4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\n",
       "5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\n",
       "6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#In the \"cars\" data frame, change the \"mpg\" (miles per gallon) column to miles per liter and then change it back\n",
    "#1 gallon is 3.78541178 liters\n",
    "cars$mpg <- cars$mpg/3.78541178\n",
    "SparkR::head(cars)\n",
    "#Change it back\n",
    "cars$mpg <- cars$mpg*3.78541178\n",
    "SparkR::head(cars)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now try the same on the `flowers` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#In the \"flowers\" data frame, change the \"Petal_Length\" column from centimeters to millimeters and then change it back\n",
    "#1 centimeter is 10 millimeters\n",
    "#type your code here\n",
    "\n",
    "#Change it back\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utilizing SQL queries in SparkR\n",
    "You can also **utilize SQL queries in SparkR**, thanks to the SQL Context created. Before utilizing SQL queries, you need to register your data frames as `tempTables`. Let's do this right now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Create a temporary SQL table called \"cars\" using our SparkR data frame \"cars\"\n",
    "registerTempTable(cars,\"cars\")\n",
    "#Create a temporary SQL table called \"flowers\" using our SparkR data frame \"flowers\"\n",
    "registerTempTable(flowers,\"flowers\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have our temporary tables, we can perform queries using the `sql` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   mpg cyl  disp  hp drat   wt  qsec vs am gear carb\n",
       "1 18.7   8 360.0 175 3.15 3.44 17.02  0  0    3    2\n",
       "2 14.3   8 360.0 245 3.21 3.57 15.84  0  0    3    4\n",
       "3 16.4   8 275.8 180 3.07 4.07 17.40  0  0    3    3\n",
       "4 17.3   8 275.8 180 3.07 3.73 17.60  0  0    3    3\n",
       "5 15.2   8 275.8 180 3.07 3.78 18.00  0  0    3    3\n",
       "6 10.4   8 472.0 205 2.93 5.25 17.98  0  0    3    4"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Select the first six rows from the \"cars\" data frame where the value of the \"cyl\" column is greater than 6\n",
    "SparkR::head(sql(sqlContext, \"SELECT * FROM cars WHERE cyl > 6\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, try performing a query on the `flowers` data frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Select the first six rows from the \"flowers\" data frame where the value of the \"Petal_Length\" column is greater than 1\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">That wraps up our Hands-on Lab 2. Remember that if you want any more information on SparkR, you can visit their documentation page at https://spark.apache.org/docs/latest/sparkr.html </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hands-on Lab 3: Linear Models in SparkR <a id=\"hl3\"></a>\n",
    "In this hands-on lab, we will go over Generalized Linear Models in SparkR. Generalized Linear Models in SparkR are constructed similar to their R counterparts, but still are different in their core.\n",
    "\n",
    "GLMs, as they are called in SparkR, are based on the **`MLlib`** Spark library for Machine Learning. SparkR makes its use easier as the functions to interact with these models are largely based on the pre-existing **R** functions.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">This Hands-on Lab assumes you have already created the SQL Context and loaded the `mtcars` and `iris` data frames from R. If for some reason you have not done so yet, execute the code block below.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning message:\n",
      "In FUN(X[[i]], ...): Use Sepal_Length instead of Sepal.Length  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Sepal_Width instead of Sepal.Width  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Petal_Length instead of Petal.Length  as column nameWarning message:\n",
      "In FUN(X[[i]], ...): Use Petal_Width instead of Petal.Width  as column name"
     ]
    }
   ],
   "source": [
    "#Initiate our SQL Context\n",
    "sqlContext <- sparkRSQL.init(sc)\n",
    "#Create a Data Frame called \"cars\" from the mtcars dataset\n",
    "cars <- createDataFrame(sqlContext,mtcars)\n",
    "#Create a Data Frame called \"flowers\" from the iris dataset\n",
    "flowers <- createDataFrame(sqlContext,iris)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Gaussian Regression Model\n",
    "To create a Gaussian Regression Model, we utilize the general `glm` function, **passing the value `gaussian` to the `family` parameter**, indicating that it is a Gaussian model.\n",
    "\n",
    "`glm` understands most of **R**'s formula operators, such as **~, +, -, . and :**. We can use them to easily create the model, like so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a GLM of the Gaussian family of models, using the formula that has \"mpg\" as the response variable and\n",
    "#\"hp\" and \"cyl\" as the predictors.\n",
    "model <- SparkR::glm(mpg ~ hp + cyl, data = cars, family = \"gaussian\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can check the data for this model in a easy-to-read manner using the `summary` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$devianceResiduals\n",
       " Min       Max     \n",
       " -4.494752 7.293354\n",
       "\n",
       "$coefficients\n",
       "            Estimate   Std. Error t value   Pr(>|t|)    \n",
       "(Intercept) 36.90833   2.190799   16.84698  2.220446e-16\n",
       "hp          -0.0191217 0.01500073 -1.274718 0.2125285   \n",
       "cyl         -2.264694  0.5758892  -3.932516 0.0004803752\n"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrieve the data from our model\n",
    "SparkR::summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we know how to create this model, we can **use it for predicting data points** using the `predict` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "   mpg prediction\n",
       "1 21.0   21.21678\n",
       "2 21.0   21.21678\n",
       "3 22.8   26.07124\n",
       "4 21.4   21.21678\n",
       "5 18.7   15.44448\n",
       "6 18.1   21.31239"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create predictions based on the model created\n",
    "predictions <- SparkR::predict(model, newData = cars)\n",
    "SparkR::head(select(predictions, \"mpg\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to create a Gaussian model, try it using the `flowers` data set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a Gaussian GLM, using the formula that has \"Sepal_Length\" as the response variable and \"Sepal_Width\" and \"Species\"\n",
    "#as the predictor\n",
    "#type your code here\n",
    "\n",
    "#Retrieve the data from our model\n",
    "#type your code here\n",
    "\n",
    "#Create predictions based on the model created\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a Binomial Regression Model\n",
    "Creating a Binomial Regression Model is simple - you just pass the value `binomial` to the `family` parameter of the `glm` function, like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a Binomial GLM, using the formula that has \"am\" as the response variable and \"hp\", \"mpg\" and \"wt\" as the predictors\n",
    "model <- SparkR::glm(am ~ hp + mpg + wt, data = cars, family = \"binomial\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As seen before, we can retrieve data from our model using the `summary` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "$coefficients\n",
       "                Estimate\n",
       "(Intercept) -15.72136618\n",
       "hp            0.08389344\n",
       "mpg           1.22930192\n",
       "wt           -6.95492385\n"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Retrieve data from our model\n",
    "SparkR::summary(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then, of course, **predict data points using our binomial regression model**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "  am prediction\n",
       "1  1          1\n",
       "2  1          0\n",
       "3  1          1\n",
       "4  0          0\n",
       "5  0          0\n",
       "6  0          0"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create predictions based on the model created\n",
    "predictions <- SparkR::predict(model, newData = cars)\n",
    "SparkR::head(select(predictions, \"am\", \"prediction\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that you know how to build a binomial regression model, you can try a different model on the `cars` dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Create a Binomial GLM, using the formula that has \"vs\" as the response variable and \"drat\" ,\"disp\" and \"gear\" as\n",
    "#the predictors\n",
    "#type your code here\n",
    "\n",
    "#Retrieve data from our model\n",
    "#type your code here\n",
    "\n",
    "#Create predictions based on the model created\n",
    "#type your code here"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\" style=\"margin-top: 20px\">That wraps up our Hands-on Lab 3. Remember that if you want any more information on SparkR, you can visit their documentation page, at https://spark.apache.org/docs/latest/sparkr.html</div>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "R",
   "language": "",
   "name": "ir"
  },
  "language_info": {
   "codemirror_mode": "r",
   "file_extension": ".r",
   "mimetype": "text/x-r-source",
   "name": "R",
   "pygments_lexer": "r"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
